library(readtext)
library(tm)
library(SnowballC)
library(lsa)
install.packages("lsa")
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("*.txt")
query <- "Enterprise Resource Planning"
# Corpus Creation
docs <- Corpus(VectorSource(c(documents$text,query)))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Enterprise Resource Planning"
# Corpus Creation
docs <- Corpus(VectorSource(c(documents$text,query)))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
DTM <- removeSparseTerms(DTM,sparse = 0.98)
m <- as.matrix(DTM)
colnames(m) <- c(documents$doc_id,"query")
N <- nrow(documents)
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
doc.scores <- cosine(query.vector,doc.vector)
result <- doc.scores
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = T),]
# Top 5 Simmilar Documents
head(result)
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Dubai"
# Corpus Creation
docs <- Corpus(VectorSource(c(documents$text,query)))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
DTM <- removeSparseTerms(DTM,sparse = 0.98)
m <- as.matrix(DTM)
colnames(m) <- c(documents$doc_id,"query")
N <- nrow(documents)
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
doc.scores <- cosine(query.vector,doc.vector)
result <- doc.scores
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = T),]
# Top 5 Simmilar Documents
head(result)
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Dubai"
# Corpus Creation
docs <- Corpus(VectorSource(c(documents$text,query)))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
DTM <- removeSparseTerms(DTM,sparse = 0.98)
# Converting DTM into Matrix
m <- as.matrix(DTM)
# Setting Column Names to the Matrix m
colnames(m) <- c(documents$doc_id,"query")
# Counting Number of Documents
N <- nrow(documents)
# Data Separation/Split
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
doc.scores <- cosine(query.vector,doc.vector)
result <- doc.scores
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = T),]
# Top 5 Simmilar Documents
head(result)
install.packages("readtext")
install.packages("readtext")
install.packages("readtext")
install.packages("tm")
install.packages("SnowballC")
install.packages("lsa")
install.packages("readtext")
install.packages("proxy")
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# Corpus Creation
docs <- Corpus(VectorSource(c(documents$text,query)))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
DTM <- removeSparseTerms(DTM,sparse = 0.98)
# Converting DTM into Matrix
m <- as.matrix(DTM)
# Setting Column Names to the Matrix m
colnames(m) <- c(documents$doc_id,"query")
# Counting Number of Documents
N <- nrow(documents)
# Data Separation/Split
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
doc.scores <- cosine(query.vector,doc.vector)
result <- doc.scores
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = T),]
# Top 5 Simmilar Documents
head(result)
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
head(documents)
View(documents)
View(documents)
View(query)
?removeSparseTerms()
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# View input data
View(documents)
View(query)
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
#DTM <- removeSparseTerms(DTM,sparse = 0.98)
# Converting DTM into Matrix
m <- as.matrix(DTM)
# Setting Column Names to the Matrix m
colnames(m) <- c(documents$doc_id,"query")
# Counting Number of Documents
N <- nrow(documents)
# Data Separation/Split
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
doc.scores <- cosine(query.vector,doc.vector)
result <- doc.scores
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = T),]
# Top 5 Simmilar Documents
head(result)
?TermDocumentMatrix()
View(m)
View(DTM)
# Top 5 Simmilar Documents
head(result)
# Top 5 Simmilar Documents
head(result)
X <- c(1,2,3,4,5,6)
X <- c(1,2,3,4,5,6,"A")
View(X)
X <- c(1,2,3,4,5)
Name <- c(“A”,”B”,”C”,”D”,”E”)
X <- c(1,2,3,4,5)
Name <- c("A","B","C","D","E")
X <- c(1,2,3,4,5)
X_Name <- c("A","B","C","D","E")
View(X)
View(X_Name)
Y <- c(6,7,8,9,10)
Y_Name <- c("F","G","H","I","J")
View(Y)
View(Y_Name)
cbind(X,Y)
rbind(X,Y)
?rbind()
??rbind()
?rbind
rbind(X,Y)
cbind(X,Y)
?order
order(Y,decreasing = T)
?seq()
seq(1, 9, by = 2)
seq(17)
seq(5)
summary()
?summary()
summary(X)
summary(X_Name)
X_Name <- c("A","B","A","D","A")
X_Name <- c("A","B","A","D","A")
summary(X_Name)
?paste()
paste(X,Y,sep = "|")
paste(x,y, '  |  ')
paste(x,y, '  |  ')
x <- "This is "
y <- "a R Programming "
paste(x,y, '  |  ')
paste(x,y, sep='  |  ')
?print()
setwd("~/Desktop/Information Retrival/documents")
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(topicmodels)
# Load Documents
setwd("C:/Users/Ankit/Desktop/Search Relavant/documents")
setwd("C:/Users/Ankit/Desktop/Search Relavant/documents")
library(readtext)
library(tm)
library(SnowballC)
library(topicmodels)
# Load Documents
documents <- readtext("*.txt")
docs <- Corpus(VectorSource(documents$text))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
myStopwords <- c("can", "say","one","way","use","also","howev","tell","will")
docs <- tm_map(docs, removeWords,myStopwords)
docs <- tm_map(docs, stripWhitespace)
DTM <- TermDocumentMatrix(docs,control = list(minWordLength = 3))
m <- as.matrix(DTM)
setwd("C:/Users/Ankit/Desktop/Search Relavant/documents")
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(topicmodels)
# Load Documents
documents <- readtext("*.txt")
docs <- Corpus(VectorSource(documents$text))
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
myStopwords <- c("can", "say","one","way","use","also","howev","tell","will")
docs <- tm_map(docs, removeWords,myStopwords)
docs <- tm_map(docs, stripWhitespace)
DTM <- TermDocumentMatrix(docs,control = list(minWordLength = 3))
m <- as.matrix(DTM)
model1<-LDA(x=t(DTM),k=5,method = "VEM")
topics <- topics(model1, 1)
topics(model1, 1)
terms(model1,6)
terms(model1,3)
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# View input data
View(documents)
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, content_transformer(tolower))
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# View input data
View(documents)
View(query)
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
docs <- tm_map(docs, stemDocument)
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# View input data
View(documents)
View(query)
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
docs <- tm_map(docs, content_transformer(tolower))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, content_transformer(tolower))
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
# Load Required Pacakges
library(readtext)
library(tm)
library(SnowballC)
library(lsa)
# Load Documents
documents <- readtext("documents/*.txt")
query <- "Requirements gathering and analysis"
# View input data
View(documents)
View(query)
# Corpus Creation
input <-  c(documents$text,query)
docs <- Corpus(VectorSource(input))
# Text Cleaning
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stemDocument)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords,stopwords("en"))
docs <- tm_map(docs, stripWhitespace)
# Generating Term Document Matrix with TiIdF weight
DTM <- TermDocumentMatrix(docs,control = list(weighting = weightTfIdf,minWordLength = 3))
# Converting DTM into Matrix
m <- as.matrix(DTM)
# Setting Column Names to the Matrix m
colnames(m) <- c(documents$doc_id,"query")
View(m)
# Counting Number of Documents
N <- nrow(documents)
# Data Separation/Split
query.vector <- m[,N+1]
doc.vector <- m[,1:N]
# Cosine Similarity
result <- cosine(query.vector,doc.vector)
result=as.data.frame(cbind(documents$doc_id,result))
rownames(result)<-NULL
colnames(result)<-c("Document",'Score')
result=result[order(result$Score,decreasing = TRUE),]
# Top 5 Simmilar Documents
head(result)
packrat::init(options = list(auto.snapshot = TRUE, use.cache = TRUE))
install.packages("tm")
